{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle as pkl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'/home/shensy/Code/python/doduo/data/turl_dataset/train.table_col_type.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2728176-1', 'slovak parliamentary election, 1990', 2728176, 'results', '', ['party'], [[[[0, 0], [1314381, 'Public Against Violence']], [[1, 0], [482855, 'Christian Democratic Movement']], [[2, 0], [1381773, 'Slovak National Party']], [[3, 0], [11450391, 'Communist Party of Slovakia']], [[4, 0], [40341252, 'Coexistence – Hungarian Christian Democratic Movement']], [[5, 0], [1973976, 'Democratic Party']], [[6, 0], [47633978, 'Party of Greens']], [[9, 0], [2728242, 'Freedom Party']], [[14, 0], [765352, 'Czechoslovakian Socialist Party']]]], [['organization.organization']]]\n"
     ]
    }
   ],
   "source": [
    "with open (path, \"r\") as f:\n",
    "    turl_type_test = json.load(f)\n",
    "    print(turl_type_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'/home/shensy/Code/python/doduo/data/ssy_test.coltype.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        table_id                                             labels  \\\n",
      "0      6675886-1                      [sports.sports_league_season]   \n",
      "1      6675886-1                                    [people.person]   \n",
      "2      6675886-1                        [organization.organization]   \n",
      "3      6675886-1                      [business.business_operation]   \n",
      "4      6675886-1                               [automotive.company]   \n",
      "...          ...                                                ...   \n",
      "1870  40723972-2  [location.administrative_division, location.lo...   \n",
      "1871  40723972-2                        [organization.organization]   \n",
      "1872  40723972-2  [government.political_party, organization.orga...   \n",
      "1873  40723972-2                        [organization.organization]   \n",
      "1874  40723972-2  [government.political_party, organization.orga...   \n",
      "\n",
      "                                                   data  \\\n",
      "0     1998 1999 2000 2001 2002 2003 2004 2005 2006 2...   \n",
      "1     Jackie Stewart Adrian Fernández Adrian Fernánd...   \n",
      "2     Mecom Racing Team Patrick Racing Patrick Racin...   \n",
      "3     Lola Reynard Reynard Lola Lola Lola Dallara Da...   \n",
      "4     Ford Ford - Cosworth Ford - Cosworth Ford - Co...   \n",
      "...                                                 ...   \n",
      "1870  KIL JAF JAF MUL MAN JAF MAN VAV VAV MUL JAF JA...   \n",
      "1871  ITAK ITAK EPRLF ACMC TELO ITAK TELO TULF EPRLF...   \n",
      "1872  TNA TNA TNA TNA UPFA TNA TNA TNA TNA TNA UPFA ...   \n",
      "1873  ITAK ITAK EPRLF ACMC TELO ITAK TELO TULF EPRLF...   \n",
      "1874  TNA TNA TNA TNA UPFA TNA TNA TNA TNA TNA UPFA ...   \n",
      "\n",
      "                                              label_ids            header  \\\n",
      "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     season season   \n",
      "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     driver driver   \n",
      "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         team team   \n",
      "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   chassis chassis   \n",
      "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     engine engine   \n",
      "...                                                 ...               ...   \n",
      "1870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          district   \n",
      "1871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     elected party   \n",
      "1872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  elected alliance   \n",
      "1873  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     current party   \n",
      "1874  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  current alliance   \n",
      "\n",
      "                                                   type  \n",
      "0     {'Champ_Car_season': 5, 'IndyCar_Series_season...  \n",
      "1     {'http://dbpedia.org/ontology/Person': 10, 'ht...  \n",
      "2     {'http://dbpedia.org/ontology/SoccerClub': 14,...  \n",
      "3     {'http://dbpedia.org/ontology/Company': 15, 'h...  \n",
      "4     {'http://dbpedia.org/ontology/Company': 15, 'h...  \n",
      "...                                                 ...  \n",
      "1870  {'http://dbpedia.org/ontology/Place': 21, 'htt...  \n",
      "1871  {'http://dbpedia.org/ontology/Organisation': 1...  \n",
      "1872  {'http://dbpedia.org/ontology/Organisation': 1...  \n",
      "1873  {'http://dbpedia.org/ontology/Organisation': 1...  \n",
      "1874  {'http://dbpedia.org/ontology/Organisation': 1...  \n",
      "\n",
      "[1875 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "with open (path, \"rb\") as f:\n",
    "    turl_type_test = pkl.load(f)\n",
    "print(turl_type_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open (\"mapping_dict.json\", \"r\") as f:\n",
    "        mdict = json.load(f)\n",
    "except:\n",
    "    mdict={}\n",
    "\n",
    "\n",
    "with open (path, \"rb\") as f:\n",
    "    turl_type_test = pkl.load(f)\n",
    "    tlabels = turl_type_test[\"train\"][\"labels\"].tolist()\n",
    "    nlabels = [np.where(np.array(nl) == 1)[0].tolist() for nl in turl_type_test[\"train\"][\"label_ids\"].tolist()]\n",
    "    for i,t in enumerate(tlabels):\n",
    "        flag = True\n",
    "        while flag:\n",
    "            flag=False\n",
    "            for e in t:\n",
    "                if e in mdict.keys():\n",
    "                    t.remove(e)\n",
    "                    nlabels[i].remove(mdict[e])\n",
    "                    flag=True\n",
    "    \n",
    "    \n",
    "    for i in range(len(tlabels)):\n",
    "        if len(tlabels[i])==1:\n",
    "            mdict[tlabels[i][0]]=nlabels[i][0]\n",
    "        elif len(tlabels[i])>1:\n",
    "            print(tlabels[i], nlabels[i])\n",
    "    print(len(mdict))\n",
    "    # res = [list(zip(temp[0],temp[1])) for temp in comb]\n",
    "    \n",
    "    # formatted_res = res[0]\n",
    "    # for ele in res[1:]:\n",
    "    #     formatted_res += ele\n",
    "with open (\"mapping_dict.json\", \"w\") as f:\n",
    "    json.dump(mdict,f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "class DbSparql:\n",
    "    def __init__(self,endpoint):\n",
    "        self.endpoint=endpoint\n",
    "        self.sparql = SPARQLWrapper(self.endpoint)\n",
    "        self.sparql.setReturnFormat(JSON)\n",
    "        self.sparql.setTimeout(10)\n",
    "\n",
    "    def set_query(self,query):\n",
    "        self.sparql.setQuery(query)\n",
    "    def get_type(self,mention):\n",
    "        if not mention:\n",
    "            return []\n",
    "        result_list = []  \n",
    "        queryTypes = \"\"\"\n",
    "                SELECT DISTINCT str(?mtype) as ?mtype \n",
    "                WHERE {{\n",
    "                    {{\n",
    "                        ?val foaf:isPrimaryTopicOf <http://en.wikipedia.org/wiki/%s>.\n",
    "                        ?val rdf:type ?mtype.\n",
    "                    }}\n",
    "                FILTER (strstarts(str(?mtype),'http://dbpedia.org/ontology/'))\n",
    "                FILTER (strstarts(str(?val),'http://dbpedia.org/resource/'))\n",
    "                }}\n",
    "            \"\"\" % mention\n",
    "\n",
    "        self.set_query(queryTypes)\n",
    "        results = self.sparql.query().convert()\n",
    "\n",
    "        for result in results[\"results\"][\"bindings\"]:\n",
    "            result_list.append(result[\"mtype\"][\"value\"])\n",
    "\n",
    "        if len(result_list) == 0:\n",
    "            return []\n",
    "\n",
    "        return result_list\n",
    "\n",
    "\n",
    "class CaliSparql:\n",
    "\n",
    "    def __init__(self,endpoint):\n",
    "        self.endpoint=endpoint\n",
    "        self.sparql = SPARQLWrapper(self.endpoint)\n",
    "        self.sparql.setReturnFormat(JSON)\n",
    "        self.sparql.setTimeout(10)\n",
    "\n",
    "    def set_query(self,query):\n",
    "        self.sparql.setQuery(query)\n",
    "\n",
    "\n",
    "    def get_type(self,mention):\n",
    "        if not mention:\n",
    "            return []\n",
    "        result_list = []\n",
    "        queryTypes = \"\"\"\n",
    "                SELECT DISTINCT str(?mtype) as ?mtype \n",
    "                WHERE {{\n",
    "                    {{\n",
    "                        <http://caligraph.org/resource/%s> rdf:type ?mtype.\n",
    "                    }}\n",
    "                FILTER (strstarts(str(?mtype),'http://caligraph.org/ontology/'))\n",
    "                }}\n",
    "            \"\"\" % mention\n",
    "\n",
    "        self.set_query(queryTypes)\n",
    "        results = self.sparql.query().convert()\n",
    "\n",
    "\n",
    "        for result in results[\"results\"][\"bindings\"]:\n",
    "            result_list.append(result[\"mtype\"][\"value\"])\n",
    "\n",
    "        if len(result_list) == 0:\n",
    "            return []\n",
    "\n",
    "        return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def wiki_id2url(pgid:int):   \n",
    "    pgid= str(pgid)\n",
    "    x = requests.get('https://en.wikipedia.org/w/api.php?action=query&prop=info&pageids=%s&inprop=url&format=json' % pgid)\n",
    "    return x.json()[\"query\"][\"pages\"][pgid][\"fullurl\"]\n",
    "\n",
    "def wiki_url2type(wikiurl:str):\n",
    "    dbq=DbSparql(endpoint='https://dbpedia.org/sparql')\n",
    "    caliq=CaliSparql(endpoint='http://caligraph.org/sparql')\n",
    "    return dbq.get_type(wikiurl.split(\"/\")[-1])+caliq.get_type(wikiurl.split(\"/\")[-1])\n",
    "\n",
    "turl_dataset = json.load(open('/home/shensy/Code/python/doduo/data/turl_dataset/test.table_col_type.json',\"r\"))\n",
    "idx_dict = json.load(open('./mapping_dict.json',\"r\"))\n",
    "\n",
    "try:\n",
    "    wiki_id2u = json.load(open('/home/shensy/Code/python/doduo/data/wiki_id2url.json',\"r\"))\n",
    "except:\n",
    "    wiki_id2u = {}\n",
    "try:\n",
    "    wiki2heter = json.load(open('/home/shensy/Code/python/doduo/data/wikiurl2type.json',\"r\"))\n",
    "except:\n",
    "    wiki2heter = {}\n",
    "\n",
    "fineg_pre_list = []\n",
    "for table in turl_dataset[:3]:\n",
    "    [table_idx, page_title, table_id, headline, table_name, header, data, gd_header] = table\n",
    "    \n",
    "    table_id  = [table_idx for i in range(len(data))]\n",
    "    list_col_data = [[] for i in range(len(data))]\n",
    "    list_type_dict = [{} for i in range(len(data))]\n",
    "    labels = gd_header\n",
    "    header = header\n",
    "    \n",
    "    for i,row in enumerate(data):\n",
    "        for cell in row:\n",
    "            list_col_data[i].append(cell[-1][-1]) \n",
    "            if cell[-1][0] in wiki_id2u.keys():\n",
    "                key = wiki_id2u[cell[-1][0]]\n",
    "            else:\n",
    "                try:\n",
    "                    key = wiki_id2url(cell[-1][0])\n",
    "                except:\n",
    "                    continue\n",
    "                wiki_id2u[cell[-1][0]] = key\n",
    "            if key in wiki2heter.keys():\n",
    "                type_set = wiki2heter[key]\n",
    "            else:\n",
    "                try:\n",
    "                    type_set = wiki_url2type(key)\n",
    "                except:\n",
    "                    continue\n",
    "                wiki2heter[key] = type_set\n",
    "            for t in type_set:\n",
    "                try:\n",
    "                    list_type_dict[i][t]+=1\n",
    "                except:\n",
    "                    list_type_dict[i][t]=1\n",
    "    \n",
    "    list_col_data = [' '.join(l) for l in list_col_data]\n",
    "    label_ids = [np.zeros(len(idx_dict)) for i in range(len(data))]\n",
    "    for i, ids in enumerate(label_ids):\n",
    "        ids[[idx_dict[t] for t in gd_header[i]]] = 1\n",
    "    fineg_pre_list += zip(table_id,labels,list_col_data,label_ids,header,list_type_dict)\n",
    "\n",
    "ssy_df = pd.DataFrame(fineg_pre_list,\n",
    "                    columns=[\n",
    "                        \"table_id\", \"labels\", \"data\",\n",
    "                        \"label_ids\", \"header\", \"entities\"\n",
    "                    ])\n",
    "\n",
    "with open('/home/shensy/Code/python/doduo/data/wiki_id2url.json',\"w\") as f:\n",
    "    json.dump(wiki_id2u,f)\n",
    "with open('/home/shensy/Code/python/doduo/data/wikiurl2type.json',\"w\") as f:\n",
    "    json.dump(wiki2heter,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%E6%AF%9B%E6%B3%BD%E4%B8%9C\n"
     ]
    }
   ],
   "source": [
    "text = \"https://zh.wikipedia.org/wiki/%E6%AF%9B%E6%B3%BD%E4%B8%9C\"\n",
    "\n",
    "print(text.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "import csv, json\n",
    "import pandas as pd\n",
    "from turtle import color\n",
    "# csv_iterator=csv.reader(open(\"./sato_cv_4.csv\",'r',encoding=\"utf8\"))\n",
    "\n",
    "# type2num=dict()\n",
    "# for line in csv_iterator:\n",
    "#     try:\n",
    "#         type2num[line[3]]=line[2]\n",
    "#     except:\n",
    "#         continue\n",
    "# print(type2num)\n",
    "\n",
    "\n",
    "x_data=[]\n",
    "y_data=[]\n",
    "x_num=[]\n",
    "res_iterator=json.load(open(\"../eval/turlturl-recl_mosato_bert_bert-base-uncased-bs16-ml-32__turl-1.00_turl-re-1.00=turl.json\",'r',encoding=\"utf8\"))\n",
    "\n",
    "print(res_iterator[\"f1_macro\"][\"ts_class_f1\"][16])\n",
    "\n",
    "# for i in range(len(res_iterator[\"f1_macro\"][\"ts_class_f1\"])):\n",
    "#     if res_iterator[\"f1_macro\"][\"ts_class_f1\"][i]<0.7:\n",
    "#         y_data.append(res_iterator[\"f1_macro\"][\"ts_class_f1\"][i])\n",
    "#         x_data.append(type2num[str(i)])\n",
    "#         x_num.append(i)\n",
    "#     print(i, res_iterator[\"f1_macro\"][\"ts_class_f1\"][i])\n",
    "\n",
    "# res=[]\n",
    "# for i in x_num:\n",
    "#     print(sum(res_iterator[\"f1_macro\"][\"confusion_matrix\"][i]),type2num[str(i)])\n",
    "#     res.append(res_iterator[\"f1_macro\"][\"confusion_matrix\"][i])\n",
    "# dat = pd.DataFrame(data=res) \n",
    "\n",
    "# dat.to_csv(\"bad_acc_sato.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1]\n",
      " [1 2]]\n"
     ]
    }
   ],
   "source": [
    "k = np.array([[[1, 0],\n",
    "        [0, 1]],\n",
    "\n",
    "       [[1, 0],\n",
    "        [0, 1]],\n",
    "\n",
    "       [[0, 1],\n",
    "        [1, 0]]])\n",
    "print(k.sum(axis=0))\n",
    "print(k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4\n",
      "2 5\n",
      "3 6\n",
      "A 10\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "a = re.compile(r\"(A|[1-9])\")\n",
    "\n",
    "for i in a.finditer(\"....123...A\"):\n",
    "    print(i.group(), i.start())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doduo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39932664a524d94cab53f4c44d679cbe80a2c2f17c62037c31679bd80183caea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
